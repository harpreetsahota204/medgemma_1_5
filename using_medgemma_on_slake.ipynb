{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harpreetsahota204/medgemma_1_5/blob/main/using_medgemma_on_slake.ipynb)\n",
    "\n",
    "\n",
    "# Using MedGemma as Remotely Sourced Zoo Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "\n",
    "dataset = load_from_hub(\n",
    "    \"Voxel51/SLAKE\",\n",
    "    name=\"SLAKE\",\n",
    "    overwrite=True,\n",
    "    max_samples=50\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Zoo Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "foz.register_zoo_model_source(\"https://github.com/harpreetsahota204/medgemma_1_5\", overwrite=True)\n",
    "\n",
    "foz.download_zoo_model(\n",
    "    \"https://github.com/harpreetsahota204/medgemma_1_5\",\n",
    "    model_name=\"google/medgemma-1.5-4b-it\", \n",
    ")\n",
    "\n",
    "model = foz.load_zoo_model(\n",
    "    \"google/medgemma-1.5-4b-it\",\n",
    "    # install_requirements=True #run this to install requirements if they're not already\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batching for Faster Inference\n",
    "\n",
    "MedGemma supports efficient batch processing for significantly faster inference on large datasets. Use `batch_size` and `num_workers` parameters in `apply_model()`:\n",
    "\n",
    "```python\n",
    "dataset.apply_model(\n",
    "    model, \n",
    "    label_field=\"predictions\",\n",
    "    batch_size=64,    # Number of images per batch\n",
    "    num_workers=8,    # Parallel data loading workers\n",
    ")\n",
    "```\n",
    "\n",
    "**Note:** The optimal `batch_size` and `num_workers` values depend on your available GPU memory and CPU resources. Start with smaller values (e.g., `batch_size=8`, `num_workers=4`) and increase as your hardware allows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use MedGemma for Classification\n",
    "\n",
    "You can use this model for a zero-shot classification task as follows, which will add a [FiftyOne Classificaton](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.Classification) to your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_system_labels = dataset.distinct(\"modality.label\")\n",
    "\n",
    "model.operation = \"classify\"\n",
    "\n",
    "model.prompt = \"As a medical expert your task is to classify this image into exactly one of the following types: \" + \", \".join(body_system_labels)\n",
    "\n",
    "# Use batching for faster inference - adjust batch_size and num_workers based on your resources\n",
    "dataset.apply_model(\n",
    "    model, \n",
    "    label_field=\"pred_modality\",\n",
    "    batch_size=8,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.first()['modality']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the model prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.first()['pred_modality.classifications']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using MedGemma for VQA\n",
    "\n",
    "You can use the model for visual question answering as shown below. This example will use the same prompt on each [Sample](https://docs.voxel51.com/api/fiftyone.core.sample.html#module-fiftyone.core.sample) in the Dataset. Note the default system prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can customize the system prompt as follows to guide the model's response. Note that we are using an existing field on the sample by passing `prompt_field=\"question\"` into the [`apply_model`](https://docs.voxel51.com/api/fiftyone.core.dataset.html) method of the [Dataset](https://docs.voxel51.com/api/fiftyone.core.dataset.html).\n",
    "\n",
    "Note that if you want to parse the model output as a [FiftyOne Classification](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.Classification) then you need to very specifically prompt the model to output in a way that this integration expects, that is:\n",
    "\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"classifications\": [\n",
    "        {\n",
    "            \"label\": \"your answer to the question\",\n",
    "            ...,\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Notice below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.operation=\"classify\"\n",
    "\n",
    "model.system_prompt = \"\"\"You have expert-level medical knowledge in radiology, histopathology, ophthalmology, and dermatology.\n",
    "\n",
    "You will be asked a question and are required to provide your answer. Your answer must be in the following format:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"classifications\": [\n",
    "        {\n",
    "            \"label\": \"your answer to the question\",\n",
    "            ...,\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Always return your response as valid JSON wrapped in ```json blocks and respond only with one answer.\n",
    "\"\"\"\n",
    "\n",
    "dataset.apply_model(\n",
    "    model, \n",
    "    label_field=\"pred_answer_6\", \n",
    "    prompt_field=\"question_6\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.first()['question_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.first()['answer_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.first()['pred_answer_6.classifications']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For open-ended generation, you can use `vqa` mode. Note that with both modes you can use a single question on each sample as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.system_prompt = None # we need to clear the custom system prompt  \n",
    "\n",
    "model.operation=\"vqa\"\n",
    "\n",
    "model.prompt = \"Describe any anomolies in this the image that you observe.\"\n",
    "\n",
    "dataset.apply_model(model, label_field=\"open_response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.first()[\"open_response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or use you can use a Sample field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.operation=\"vqa\"\n",
    "\n",
    "dataset.apply_model(\n",
    "    model, \n",
    "    label_field=\"answer_5_vqa\", \n",
    "    prompt_field=\"question_5\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.first()[\"question_5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.first()[\"answer_5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.first()[\"answer_5_vqa\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use MedGemma for Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.operation=\"detect\"\n",
    "\n",
    "labels = dataset.distinct(\"detections.detections.label\")\n",
    "\n",
    "labels_str = \", \".join(labels)\n",
    "\n",
    "prompt = f\"\"\"Locate the following in this scan: {labels_str}. Output the final answer in the format \"Final Answer: X\" where X is a JSON list of objects. \n",
    "The object needs a \"box_2d\" and \"label\" key. \n",
    "If the object is not present in the scan, skip it and don't output anything for that object.\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.apply_model(\n",
    "    model,\n",
    "    label_field=\"pred_detection\",\n",
    "    batch_size=8,\n",
    "    num_workers=4,\n",
    "    skip_failures=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model\n",
    "\n",
    "You can use FiftyOne's [Evaluation API](https://docs.voxel51.com/user_guide/evaluation.html) to evaluate model performance via the SDK. \n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dataset.evaluate_classifications(\n",
    "    \"pred_modality\",\n",
    "    gt_field=\"modality\",\n",
    "    eval_key=\"eval_simple\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then review as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = results.plot_confusion_matrix()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can, of course, do all of this in the App as shown here:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
